{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Recurrent Neural Network\n",
    "### Justin Marlor & Habit Blunk\n",
    "##### *Colorado State University*\n",
    "\n",
    "This is our notebook that automatically copies data from [this dataset hosted on Kaggle](https://www.kaggle.com/datasets/mczielinski/bitcoin-historical-data), then throws it into various neural networks and predicts the price of Bitcoin.\n",
    "\n",
    "This dataset is centered around the price of Bitcoin. The original dataset included 1 minute interval measures of the 'High', 'Low', 'Open', 'Close', 'Volume', as well as the UNIX timestamp and a column 'datetime'. The original dataset was over contained over 7,000,000 rows. \n",
    "We used a recurrent neural network (RNN), as well as a long short-term memory (LSTM) model to make price predictions for the near future. The main motivation to use this model comes from the RNN's ability to effectively do time-series forecasting since it retains sequential history and can handle long-term trends better than other models.\n",
    "\n",
    "### Methodology\n",
    "To reduce noise and redundancy, the data was aggregated by selecting only rows recorded at 23:59 each day, creating a daily time series. The features Open, High, and Low were dropped, retaining only the Close price and Volume for prediction. Before training, the data was standardized using StandardScaler to improve model convergence and stability. Sequences of 60 timesteps (approximately two months of data) were constructed as input features, and the Close price at the next timestep served as the prediction target. These sequences were wrapped in a custom TimeSeriesDataset class and split into training and validation sets using a 95/5 ratio. We trained and evaluated two models: a basic Recurrent Neural Network (RNN) and a Long Short-Term Memory (LSTM) network. Both used 64 hidden units and a single recurrent layer. The models were trained over a range of epochs [2, 4, 8, 16, 32, 64, 128], and the model achieving the lowest Mean Squared Error (MSE) on the training set was saved. Finally, the best-performing model was used to predict future Close prices for 10 days into the future. Since the model output was in normalized form, predictions were denormalized using the inverse transform of the original scaler to obtain interpretable price values.\n",
    "\n",
    "To run it:\n",
    "\n",
    "1. Run the script located in this repository at `./env-script`. This will set up your virtual environment. \n",
    "\n",
    "    `./env-script`\n",
    "    ```bash\n",
    "    #!/bin/bash\n",
    "    [ ! -d \"venv\" ] && python -m venv venv\n",
    "    source \"venv/bin/activate\" && pip install -r \"dependencies.txt\" && python -m ipykernel install --user --name=venv && jupyter notebook\n",
    "    ```\n",
    "   `./add-run-kaggle-bitcoin` \n",
    "   ```bash\n",
    "    #!/bin/bash\n",
    "    [ ! -d \"kaggle-bitcoin\" ] && git clone https://github.com://chabitblunk/kaggle-bitcoin.git\n",
    "    cd kaggle-bitcoin || exit\n",
    "    poetry install && poetry run python kaggle_bitcoin/kaggle_update_bitcoin.py \n",
    "   ```\n",
    "    `./dependencies.txt`\n",
    "      ```\n",
    "      anyio==4.4.0\n",
    "      argon2-cffi==23.1.0\n",
    "      argon2-cffi-bindings==21.2.0\n",
    "      arrow==1.3.0\n",
    "      asttokens==2.4.1\n",
    "      async-lru==2.0.4\n",
    "      attrs==24.2.0\n",
    "      babel==2.16.0\n",
    "      beautifulsoup4==4.12.3\n",
    "      bleach==6.1.0\n",
    "      build==1.2.2.post1\n",
    "      CacheControl==0.14.2\n",
    "      certifi==2024.8.30\n",
    "      cffi==1.17.1\n",
    "      charset-normalizer==3.3.2\n",
    "      cleo==2.1.0\n",
    "      comm==0.2.2\n",
    "      contourpy==1.3.1\n",
    "      crashtest==0.4.1\n",
    "      cryptography==44.0.2\n",
    "      cycler==0.12.1\n",
    "      debugpy==1.8.5\n",
    "      decorator==5.1.1\n",
    "      defusedxml==0.7.1\n",
    "      distlib==0.3.9\n",
    "      dulwich==0.22.8\n",
    "      executing==2.1.0\n",
    "      fastjsonschema==2.20.0\n",
    "      filelock==3.18.0\n",
    "      findpython==0.6.3\n",
    "      fonttools==4.56.0\n",
    "      fqdn==1.5.1\n",
    "      fsspec==2025.3.2\n",
    "      h11==0.14.0\n",
    "      httpcore==1.0.5\n",
    "      httpx==0.27.2\n",
    "      idna==3.8\n",
    "      installer==0.7.0\n",
    "      ipykernel==6.29.5\n",
    "      ipython==8.27.0 ipython_pygments_lexers==1.1.1\n",
    "      ipywidgets==8.1.5\n",
    "      isoduration==20.11.0\n",
    "      jaraco.classes==3.4.0\n",
    "      jaraco.context==6.0.1\n",
    "      jaraco.functools==4.1.0\n",
    "      jedi==0.19.1\n",
    "      jeepney==0.9.0\n",
    "      Jinja2==3.1.4\n",
    "      joblib==1.4.2\n",
    "      json5==0.9.25\n",
    "      jsonpointer==3.0.0\n",
    "      jsonschema==4.23.0\n",
    "      jsonschema-specifications==2023.12.1\n",
    "      jupyter==1.1.1\n",
    "      jupyter-console==6.6.3\n",
    "      jupyter-events==0.10.0\n",
    "      jupyter-lsp==2.2.5\n",
    "      jupyter_client==8.6.2\n",
    "      jupyter_core==5.7.2\n",
    "      jupyter_server==2.14.2\n",
    "      jupyter_server_terminals==0.5.3\n",
    "      jupyterlab==4.3.6\n",
    "      jupyterlab_pygments==0.3.0\n",
    "      jupyterlab_server==2.27.3\n",
    "      jupyterlab_widgets==3.0.13\n",
    "      kaggle==1.6.17\n",
    "      -e git+https://github.com://chabitblunk/kaggle-bitcoin.git@382e5fe336c5cb213f32a62cf838d453498b8781#egg=kaggle_bitcoin\n",
    "      keyring==25.6.0\n",
    "      kiwisolver==1.4.8\n",
    "      MarkupSafe==2.1.5\n",
    "      matplotlib==3.10.1\n",
    "      matplotlib-inline==0.1.7\n",
    "      mistune==3.0.2\n",
    "      more-itertools==10.6.0\n",
    "      mpmath==1.3.0\n",
    "      msgpack==1.1.0\n",
    "      nbclient==0.10.0\n",
    "      nbconvert==7.16.4\n",
    "      nbformat==5.10.4\n",
    "      nest-asyncio==1.6.0\n",
    "      networkx==3.4.2\n",
    "      notebook==7.3.3\n",
    "      notebook_shim==0.2.4\n",
    "      numpy==2.1.1\n",
    "      nvidia-cublas-cu12==12.4.5.8\n",
    "      nvidia-cuda-cupti-cu12==12.4.127\n",
    "      nvidia-cuda-nvrtc-cu12==12.4.127\n",
    "      nvidia-cuda-runtime-cu12==12.4.127\n",
    "      nvidia-cudnn-cu12==9.1.0.70\n",
    "      nvidia-cufft-cu12==11.2.1.3\n",
    "      nvidia-curand-cu12==10.3.5.147\n",
    "      nvidia-cusolver-cu12==11.6.1.9\n",
    "      nvidia-cusparse-cu12==12.3.1.170\n",
    "      nvidia-cusparselt-cu12==0.6.2\n",
    "      nvidia-nccl-cu12==2.21.5\n",
    "      nvidia-nvjitlink-cu12==12.4.127\n",
    "      nvidia-nvtx-cu12==12.4.127\n",
    "      overrides==7.7.0\n",
    "      packaging==24.1\n",
    "      pandas==2.2.3\n",
    "      pandocfilters==1.5.1\n",
    "      parso==0.8.4\n",
    "      pbs-installer==2025.4.9\n",
    "      pexpect==4.9.0\n",
    "      pillow==11.2.1\n",
    "      pkginfo==1.12.1.2\n",
    "      platformdirs==4.3.2\n",
    "      poetry==2.1.0\n",
    "      poetry-core==2.1.0\n",
    "      prometheus_client==0.20.0\n",
    "      prompt_toolkit==3.0.47\n",
    "      protobuf==6.30.2\n",
    "      psutil==6.0.0\n",
    "      ptyprocess==0.7.0\n",
    "      pure_eval==0.2.3\n",
    "      pycparser==2.22\n",
    "      Pygments==2.18.0\n",
    "      pyparsing==3.2.3\n",
    "      pyproject_hooks==1.2.0\n",
    "      python-dateutil==2.9.0.post0\n",
    "      python-json-logger==2.0.7\n",
    "      python-slugify==8.0.4\n",
    "      pytz==2024.2\n",
    "      PyYAML==6.0.2\n",
    "      pyzmq==26.2.0\n",
    "      RapidFuzz==3.13.0\n",
    "      referencing==0.35.1\n",
    "      requests==2.32.3\n",
    "      requests-toolbelt==1.0.0\n",
    "      rfc3339-validator==0.1.4\n",
    "      rfc3986-validator==0.1.1\n",
    "      rpds-py==0.20.0\n",
    "      scikit-learn==1.6.1\n",
    "      scipy==1.15.2\n",
    "      SecretStorage==3.3.3\n",
    "      Send2Trash==1.8.3\n",
    "      setuptools==74.1.2\n",
    "      shellingham==1.5.4\n",
    "      six==1.16.0\n",
    "      sniffio==1.3.1\n",
    "      soupsieve==2.6\n",
    "      stack-data==0.6.3\n",
    "      sympy==1.13.1\n",
    "      terminado==0.18.1\n",
    "      text-unidecode==1.3\n",
    "      threadpoolctl==3.6.0\n",
    "      tinycss2==1.3.0\n",
    "      tomlkit==0.13.2\n",
    "      torch==2.6.0\n",
    "      tornado==6.4.1\n",
    "      tqdm==4.66.5\n",
    "      traitlets==5.14.3\n",
    "      triton==3.2.0\n",
    "      trove-classifiers==2025.4.11.15\n",
    "      types-python-dateutil==2.9.0.20240906\n",
    "      typing_extensions==4.13.0\n",
    "      tzdata==2024.1\n",
    "      uri-template==1.3.0\n",
    "      urllib3==2.2.3\n",
    "      virtualenv==20.30.0\n",
    "      wcwidth==0.2.13\n",
    "      webcolors==24.8.0\n",
    "      webencodings==0.5.1\n",
    "      websocket-client==1.8.0\n",
    "      widgetsnbextension==4.0.13\n",
    "      zstandard==0.23.0\n",
    "    ``` \n",
    "2. Run `source ./venv/bin/activate`. This will put you in the virtual environment we have set up, so this notebook can be run on any machine so long as it has Python 3.x and can install the dependencies at `./dependencies.txt`.\n",
    "3. Paste this into `~/.config/kaggle/kaggle.json`:\n",
    "    ```json\n",
    "    {\n",
    "      \"username\": \"justinmarlor\",\n",
    "      \"key\": \"b98017f9291bfa83686f6c6780d38e04\"\n",
    "    }\n",
    "    ```\n",
    "4. Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Imports and TimeSeries class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import RNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor, seq_length, target_idx):\n",
    "        self.tensor = tensor\n",
    "        self.seq_length = seq_length\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.tensor[idx:idx + self.seq_length]\n",
    "        target = self.tensor[idx + self.seq_length, self.target_idx]\n",
    "        return seq, target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Grabbing and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(['bash', './add-run-kaggle-bitcoin'], capture_output=True,text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    df = pd.read_csv(\"kaggle-bitcoin/upload/btcusd_1-min_data.csv\", dtype={\"Volume\": float}, low_memory=False)\n",
    "    df['datetime'] = pd.to_datetime(df['Timestamp'].astype('Int64'), unit='s', errors='coerce')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the data to 1 Close value per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['datetime'].dt.time == pd.to_datetime('23:59').time()]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "We use PCA as a strategy to discover noise in our dataset, allowing us to remove unnecessary features in our dataset. This PCA shows us that there is a very similar correlation value between each of the 'open', 'high', 'low', 'close' columns. The trading volume is the other feature that has a strong correlation to the price of bitcoin. We will proceed to train the models using the two features, Close and Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "X = df.drop(columns=['Timestamp', 'datetime'])\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "loadings = pd.DataFrame(pca.components_.T, \n",
    "                        columns=['PC1', 'PC2'], \n",
    "                        index=X.columns)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Redundant Columns\n",
    "df = df.drop(columns=['Open','High','Low'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: plotting current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Close price\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['datetime'], df['Close'], label='close', color='orange')\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('price')\n",
    "plt.title('close price over time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Volume\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['datetime'], df['Volume'], label='volume', color='green')\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('volume')\n",
    "plt.title('volume over time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 4: Create tensor and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "close_idx = df.columns.get_loc('Close')\n",
    "mean = scaler.mean_[close_idx]\n",
    "std = scaler.scale_[close_idx]\n",
    "\n",
    "def create_sequences(tensor, seq_length, target_idx):\n",
    "  sequences = []\n",
    "  targets = []\n",
    "\n",
    "  for i in range(len(tensor) - seq_length):\n",
    "    seq = tensor[i:i + seq_length]\n",
    "    target_value =  tensor[i + seq_length, target_idx]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target_value) \n",
    "  return torch.stack(sequences), torch.tensor(targets).unsqueeze(1)\n",
    "\n",
    "seq_length = 60\n",
    "batch_size = 64\n",
    "dataset = TimeSeriesDataset(tensor, seq_length, close_idx)\n",
    "\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=1e-3):\n",
    "  loss_fn = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  \n",
    "  mses = []\n",
    "   \n",
    "  if isinstance(num_epochs, list):\n",
    "    for n in num_epochs:\n",
    "      for epoch in range(n):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "          pred = model(batch_X)\n",
    "          loss = loss_fn(pred.squeeze(), batch_y)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for val_X, val_y in val_loader:\n",
    "            val_pred = model(val_X)\n",
    "            val_loss += loss_fn(val_pred.squeeze(), val_y).item()\n",
    "      mses.append(total_loss / len(train_loader)) \n",
    "  else:\n",
    "    for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      for batch_X, batch_y in train_loader:\n",
    "        pred = model(batch_X)\n",
    "        loss = loss_fn(pred.squeeze(), batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "      model.eval()\n",
    "      val_loss = 0\n",
    "      with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "          val_pred = model(val_X)\n",
    "          val_loss += loss_fn(val_pred.squeeze(), val_y).item()\n",
    "    mses.append(total_loss / len(train_loader)) \n",
    "  return mses \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 5: RNN model training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 6: Training RNN Model + storing data into .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "epochs = [2, 4, 8, 16, 32, 64, 128]\n",
    "mses = train_model(rnn_model, train_loader, val_loader, num_epochs=epochs)\n",
    "print(f\"number of epochs that gives the lowest MSE: {epochs[mses.index(min(mses))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 7: To load trained RNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "rnn_model.load_state_dict(torch.load(\"rnn_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 8: Recursive prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, input_seq, steps, target_idx):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    seq = input_seq.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            pred = model(seq.unsqueeze(0))\n",
    "            pred_value = pred.item()\n",
    "            predictions.append(pred_value * std + mean)\n",
    "\n",
    "            # Create the next sequence by shifting and adding the prediction\n",
    "            new_step = seq[-1].clone()\n",
    "            new_step[target_idx] = pred_value  # Only update the target (Close price)\n",
    "            seq = torch.cat((seq[1:], new_step.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 9: How many values we will predict with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_seq = tensor[-seq_length:]  # Get the last sequence from the dataset\n",
    "future_steps = 10  # our chosen number of predictions\n",
    "normalized_predictions = predict_future(rnn_model, last_seq, steps=future_steps, target_idx=close_idx)\n",
    "print(normalized_predictions)\n",
    "print(set(normalized_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 10: LSTM model training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) \n",
    "    self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, x): \n",
    "    out, _ = self.lstm(x)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 11: Training LSTM against dataset, then saving it in a `*.pth` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_dim = tensor.shape[1], hidden_dim=64)\n",
    "mses = train_model(lstm_model, train_loader, val_loader, num_epochs=epochs)\n",
    "torch.save(lstm_model.state_dict(), \"lstm_model.pth\")\n",
    "print(f\"number of epochs that gives the lowest MSE: {epochs[mses.index(min(mses))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading trained LSTM Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "lstm_model.load_state_dict(torch.load(\"lstm_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_seq = tensor[-seq_length:]  # Get the last sequence from the dataset\n",
    "future_steps = 10  # our chosen number of predictions\n",
    "predictions = predict_future(lstm_model, last_seq, steps=future_steps, target_idx=close_idx)\n",
    "print(predictions)\n",
    "print(set(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After looking over the results from both models, we noticed that the LSTM's predictions tend to be more accurate than the RNN's. This is because it mitigates vanishing gradients between earlier and later layers in its architecture due to its gated structure, as well as it remembering long-term dependencies better than a vanilla RNN. However, this comes at the cost of taking longer to train. In our case, on an Nvidia 30-series consumer GPU, the difference was not that great. Additionally, the LSTM is not as easy to interpret, due to the complexity of its gate system.\n",
    "\n",
    "Ultimately, either model could serve as a somewhat accurate predictor of the price of Bitcoin. RNNs predict semiconstant values or short periods of time correctly, while LSTMs can predict much longer patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
