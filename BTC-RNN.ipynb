{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Bitcoin Recurrent Neural Network<strong>\n",
    "### Justin Marlor & Habit Blunk\n",
    "##### *Colorado State University*\n",
    "\n",
    "This is our notebook that automatically copies data from [this dataset hosted on Kaggle](https://www.kaggle.com/datasets/mczielinski/bitcoin-historical-data), then throws it into various neural networks and predicts the price of Bitcoin.\n",
    "\n",
    "To run it:\n",
    "\n",
    "1. Run the script located in this repository at `./env-script`. This will set up your virtual environment. \n",
    "2. Run `source ./venv/bin/activate`. This will put you in the virtual environment we have set up, so this notebook can be run on any machine so long as it has Python 3.x and can install the dependencies at `./dependencies.txt`.\n",
    "3. Paste this into `~/.config/kaggle/kaggle.json`:\n",
    "    ```json\n",
    "    {\n",
    "      \"username\": \"justinmarlor\",\n",
    "      \"key\": \"b98017f9291bfa83686f6c6780d38e04\"\n",
    "    }\n",
    "    ```\n",
    "4. Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: Imports and TimeSeries class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import GRU, RNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor, seq_length, target_idx):\n",
    "        self.tensor = tensor\n",
    "        self.seq_length = seq_length\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.tensor[idx:idx + self.seq_length]\n",
    "        target = self.tensor[idx + self.seq_length, self.target_idx]\n",
    "        return seq, target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Grabbing and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(['bash', './add-run-kaggle-bitcoin'], capture_output=True,text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    df = pd.read_csv(\"kaggle-bitcoin/upload/btcusd_1-min_data.csv\", dtype={\"Volume\": float}, low_memory=False)\n",
    "    df['datetime'] = pd.to_datetime(df['Timestamp'].astype('Int64'), unit='s', errors='coerce')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the data to 1 Close value per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['datetime'].dt.time == pd.to_datetime('23:59').time()]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA: We use PCA as a strategy to discover noise in our dataset, allowing us to remove unnecessary features in our dataset. This PCA shows us that there is a very similar correlation value between each of the 'open', 'high', 'low', 'close' columns. The trading volume is the other feature that has a strong correlation to the price of bitcoin. We will proceed to train the models using the two features, Close and Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "X = df.drop(columns=['Timestamp', 'datetime'])\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "loadings = pd.DataFrame(pca.components_.T, \n",
    "                        columns=['PC1', 'PC2'], \n",
    "                        index=X.columns)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Redundant Columns\n",
    "df = df.drop(columns=['Open','High','Low'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: plotting current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Close price\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['datetime'], df['Close'], label='close', color='orange')\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('price')\n",
    "plt.title('close price over time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Volume\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['datetime'], df['Volume'], label='volume', color='green')\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('volume')\n",
    "plt.title('volume over time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 4: Create tensor and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "close_idx = df.columns.get_loc('Close')\n",
    "\n",
    "def create_sequences(tensor, seq_length, target_idx):\n",
    "  sequences = []\n",
    "  targets = []\n",
    "\n",
    "  for i in range(len(tensor) - seq_length):\n",
    "    seq = tensor[i:i + seq_length]\n",
    "    target_value =  tensor[i + seq_length, target_idx]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target_value) \n",
    "  return torch.stack(sequences), torch.tensor(targets).unsqueeze(1)\n",
    "\n",
    "seq_length = 60\n",
    "batch_size = 64\n",
    "dataset = TimeSeriesDataset(tensor, seq_length, close_idx)\n",
    "\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=1e-3):\n",
    "  loss_fn = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  \n",
    "  mses = []\n",
    "   \n",
    "  if isinstance(num_epochs, list):\n",
    "    for n in num_epochs:\n",
    "      for epoch in range(n):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "          pred = model(batch_X)\n",
    "          loss = loss_fn(pred.squeeze(), batch_y)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for val_X, val_y in val_loader:\n",
    "            val_pred = model(val_X)\n",
    "            val_loss += loss_fn(val_pred.squeeze(), val_y).item()\n",
    "      mses.append(total_loss / len(train_loader)) \n",
    "  else:\n",
    "    for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      for batch_X, batch_y in train_loader:\n",
    "        pred = model(batch_X)\n",
    "        loss = loss_fn(pred.squeeze(), batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "      model.eval()\n",
    "      val_loss = 0\n",
    "      with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "          val_pred = model(val_X)\n",
    "          val_loss += loss_fn(val_pred.squeeze(), val_y).item()\n",
    "    mses.append(total_loss / len(train_loader)) \n",
    "  return mses \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 5: RNN model training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 6: Training RNN Model + storing data into .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "epochs = [2, 4, 8, 16, 32, 64, 128]\n",
    "mses = train_model(rnn_model, train_loader, val_loader, num_epochs=epochs) \n",
    "torch.save(rnn_model.state_dict(), \"rnn_model.pth\")\n",
    "print(f\"number of epochs that gives the lowest MSE: {epochs[mses.index(min(mses))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 7: To load trained RNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "rnn_model.load_state_dict(torch.load(\"rnn_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 8: Recursive prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, input_seq, steps, target_idx):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    seq = input_seq.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            pred = model(seq.unsqueeze(0))\n",
    "            pred_value = pred.item()\n",
    "            predictions.append(pred_value)\n",
    "\n",
    "            # Create the next sequence by shifting and adding the prediction\n",
    "            new_step = seq[-1].clone()\n",
    "            new_step[target_idx] = pred_value  # Only update the target (Close price)\n",
    "            seq = torch.cat((seq[1:], new_step.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 9: How many values we will predict with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_seq = tensor[-seq_length:]  # Get the last sequence from the dataset\n",
    "future_steps = 100  # our chosen number of predictions\n",
    "normalized_predictions = predict_future(rnn_model, last_seq, steps=future_steps, target_idx=close_idx)\n",
    "print(normalized_predictions)\n",
    "print(set(normalized_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 10: LSTM model training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) \n",
    "    self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, x): \n",
    "    out, _ = self.lstm(x)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 11: Training LSTM against dataset, then saving it in a `*.pth` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_dim = tensor.shape[1], hidden_dim=64)\n",
    "mses = train_model(lstm_model, train_loader, val_loader, num_epochs=epochs)\n",
    "torch.save(lstm_model.state_dict(), \"lstm_model.pth\")\n",
    "print(f\"number of epochs that gives the lowest MSE: {epochs[mses.index(min(mses))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading trained LSTM Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "lstm_model.load_state_dict(torch.load(\"lstm_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_seq = tensor[-seq_length:]  # Get the last sequence from the dataset\n",
    "future_steps = 100  # our chosen number of predictions\n",
    "normalized_predictions = predict_future(lstm_model, last_seq, steps=future_steps, target_idx=close_idx)\n",
    "print(normalized_predictions)\n",
    "print(set(normalized_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
