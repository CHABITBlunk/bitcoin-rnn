{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Bitcoin Recurrent Neural Network<strong>\n",
    "### Justin Marlor & Habit Blunk\n",
    "##### *Colorado State University*\n",
    "\n",
    "This is our notebook that automatically copies data from [this dataset hosted on Kaggle](https://www.kaggle.com/datasets/mczielinski/bitcoin-historical-data), then throws it into various neural networks and predicts the price of Bitcoin.\n",
    "\n",
    "To run it:\n",
    "\n",
    "1. Run the script located in this repository at `./env-script`. This will set up your virtual environment. \n",
    "2. Run `source ./venv/bin/activate`. This will put you in the virtual environment we have set up, so this notebook can be run on any machine so long as it has Python 3.x and can install the dependencies at `./dependencies.txt`.\n",
    "3. Paste this into `~/.config/kaggle/kaggle.json`:\n",
    "    ```json\n",
    "    {\n",
    "      \"username\": \"justinmarlor\",\n",
    "      \"key\": \"b98017f9291bfa83686f6c6780d38e04\"\n",
    "    }\n",
    "    ```\n",
    "4. Execute each cell in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 1: imports, grabbing, and preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import GRU, RNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor, seq_length, target_idx):\n",
    "        self.tensor = tensor\n",
    "        self.seq_length = seq_length\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.tensor[idx:idx + self.seq_length]\n",
    "        target = self.tensor[idx + self.seq_length, self.target_idx]\n",
    "        return seq, target \n",
    "\n",
    "result = subprocess.run(['bash', './add-run-kaggle-bitcoin'], capture_output=True,text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    df = pd.read_csv(\"kaggle-bitcoin/upload/btcusd_1-min_data.csv\", dtype={\"Volume\": float}, low_memory=False)\n",
    "    df['datetime'] = pd.to_datetime(df['Timestamp'].astype('Int64'), unit='s', errors='coerce')\n",
    "    df['Year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: plotting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['datetime'], df['Open'], label='open', color='blue')\n",
    "plt.plot(df['datetime'], df['Close'], label='close', color='orange')\n",
    "plt.plot(df['datetime'], df['High'], label='high', color='red')\n",
    "plt.plot(df['datetime'], df['Low'], label='low', color='green')\n",
    "\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('price')\n",
    "plt.title('ohlc time series')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: create tensor and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor = df.drop(columns=['datetime']).astype('float32').dropna()\n",
    "scaler = StandardScaler()\n",
    "normalized_values = scaler.fit_transform(df_tensor.values)\n",
    "tensor = torch.tensor(df_tensor.values, dtype=torch.float32)\n",
    "\n",
    "close_idx = df.columns.get_loc('Close')\n",
    "\n",
    "def create_sequences(tensor, seq_length, target_idx):\n",
    "  sequences = []\n",
    "  targets = []\n",
    "\n",
    "  for i in range(len(tensor) - seq_length):\n",
    "    seq = tensor[i:i + seq_length]\n",
    "    target_value =  tensor[i + seq_length, target_idx]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target_value) \n",
    "  return torch.stack(sequences), torch.tensor(targets).unsqueeze(1) \n",
    "\n",
    "seq_length = 60\n",
    "batch_size = 64\n",
    "dataset = TimeSeriesDataset(tensor, seq_length, close_idx)\n",
    "\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=4, lr=1e-3):\n",
    "  loss_fn = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    total_loss = 0 \n",
    "    for batch_X, batch_y in train_loader:\n",
    "      pred = model(batch_X)\n",
    "      loss = loss_fn(pred.squeeze(), batch_y)\n",
    "      optimizer.zero_grad() \n",
    "      loss.backward()\n",
    "      optimizer.step() \n",
    "      total_loss += loss.item() \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for val_X, val_y in val_loader:\n",
    "        val_pred = model(val_X)\n",
    "        val_loss += loss_fn(val_pred.squeeze(), val_y).item()\n",
    "    print(f\"epoch {epoch + 1}, train loss: {total_loss / len(train_loader):.4f}, val loss: {val_loss / len(val_loader):.4f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 4: building and training vanilla RNN against dataset, then saving it in a `*.pth` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "rnn_model = RNNModel(input_dim=tensor.shape[1], hidden_dim=64)\n",
    "train_model(rnn_model, train_loader, val_loader) \n",
    "torch.save(rnn_model.state_dict(), \"rnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 5: building and training LSTM against dataset, then saving it in a `*.pth` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim=1):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) \n",
    "    self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, x): \n",
    "    out, _ = self.lstm(x)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out \n",
    "\n",
    "lstm_model = LSTMModel(input_dim = tensor.shape[1], hidden_dim=64)\n",
    "train_model(lstm_model, train_loader, val_loader)\n",
    "torch.save(lstm_model.state_dict(), \"lstm_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
